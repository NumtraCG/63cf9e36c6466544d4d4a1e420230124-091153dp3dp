{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c320606",
   "metadata": {},
   "source": [
    "***GENERATED CODE FOR dp3dp PIPELINE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276141d6",
   "metadata": {},
   "source": [
    "***DON'T EDIT THIS CODE.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8498fb9",
   "metadata": {},
   "source": [
    "***CONNECTOR FUNCTIONS TO READ DATA.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs3 import HDFileSystem\n",
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class HDFSConnector:\n",
    "\n",
    "    def fetch(spark, config):\n",
    "        ################### INPUT HADOOP HOST PORT TO CONNECT WITH ###############################\n",
    "        hdfs = HDFileSystem(\n",
    "            host=eval(config)['host'], port=eval(config)['port'])\n",
    "        with hdfs.open(eval(config)['url']) as f:\n",
    "            df = pd.read_csv(f, error_bad_lines=False)\n",
    "        df = spark.createDataFrame(dfPd)\n",
    "        display(df.limit(2).toPandas())\n",
    "        return df\n",
    "\n",
    "    def put(df, spark, config):\n",
    "        return df.write.format('csv').options(header='true' if eval(config)[\"is_header\"] == \"Use Header Line\" else 'false',\n",
    "                                              delimiter=eval(config)[\"delimiter\"]).save((\"%s %s\") % (datetime.datetime.now().strftime(\"%Y-%m-%d %H.%M.%S\")+\"_\", eval(config)['url']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07283069",
   "metadata": {},
   "source": [
    "***OPERATION FUNCTIONS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836561fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from dask.dataframe import from_pandas\n",
    "import json\n",
    "\n",
    "\n",
    "def aggregation(df, functionsData, applyOn):\n",
    "    dfcp = df\n",
    "    for functionData in functionsData:\n",
    "        aggregateOn = []\n",
    "        groupOn = []\n",
    "\n",
    "        for column in functionData['aggregateOn']:\n",
    "            aggregateOn.append(column['columnName'])\n",
    "        for column in functionData['groupOn']:\n",
    "            groupOn.append(column['columnName'])\n",
    "        if functionData['aggregateFunction'] == 'count':\n",
    "            dfcp = (df.groupby(groupOn))[aggregateOn].count()\n",
    "        elif functionData['aggregateFunction'] == 'min':\n",
    "            dfcp = (df.groupby(groupOn))[aggregateOn].min()\n",
    "        elif functionData['aggregateFunction'] == 'max':\n",
    "            dfcp = (df.groupby(groupOn))[aggregateOn].max()\n",
    "        elif functionData['aggregateFunction'] == 'std':\n",
    "            dfcp = (df.groupby(groupOn))[aggregateOn].std()\n",
    "        elif functionData['aggregateFunction'] == 'mean':\n",
    "            dfcp = (df.groupby(groupOn))[aggregateOn].mean()\n",
    "        elif functionData['aggregateFunction'] == 'sum':\n",
    "            dfcp = (df.groupby(groupOn))[aggregateOn].sum()\n",
    "    return dfcp\n",
    "\n",
    "\n",
    "def runDataCleansing(sparkDf, spark, config):\n",
    "    configObj = json.loads(config)\n",
    "    sparkDf.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    df = from_pandas((sparkDf.toPandas()), npartitions=5)\n",
    "    functionList = configObj['functionsApplied']\n",
    "    Data_Cleansing_Methods = {\"replaceBy\": replaceValues,\n",
    "                              \"formula\": calculateFormula,\n",
    "                              \"aggregate\": aggregation,\n",
    "                              \"converttostringtype\": changeToString,\n",
    "                              \"editname\": renameColumns}\n",
    "    for function in functionList:\n",
    "        function['functionName']\n",
    "        df = Data_Cleansing_Methods[function['functionName']](df, function['functionsData'],\n",
    "                                                              function['applyOn'])\n",
    "    sparkDf = spark.createDataFrame(df.compute())\n",
    "\n",
    "    display(sparkDf.limit(2).toPandas())\n",
    "    return sparkDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b4899",
   "metadata": {},
   "source": [
    "***CONNECTOR FUNCTIONS TO WRITE DATA.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class NumtraConnector:\n",
    "\n",
    "    def put(inStages, inStagesData, stageId, spark, config):\n",
    "        path = eval(config)['server_url']\n",
    "        baseType = eval(config)['baseType']\n",
    "        results_url = eval(config)['results_url']\n",
    "        server = eval(config)['server']\n",
    "        originalfile = eval(config)['orignalKey']\n",
    "        eval(config)['pathOnly']\n",
    "        filename = eval(config)['filename']\n",
    "        eval(config)['ser']\n",
    "        eval(config)['user']\n",
    "        eval(config)['password']\n",
    "        eval(config)['authSource']\n",
    "        eval(config)['user_id']\n",
    "        eval(config)['parent_id']\n",
    "        eval(config)['project_id']\n",
    "        time = str(int(datetime.datetime.now().timestamp()))\n",
    "\n",
    "        inStagesData[inStages[0]]\n",
    "\n",
    "        print(path)\n",
    "        print(baseType)\n",
    "        print(results_url)\n",
    "        print(server)\n",
    "        print(originalfile)\n",
    "        print(filename)\n",
    "\n",
    "        args = {\n",
    "            'url': path,\n",
    "            'baseType': baseType,\n",
    "            'originalfile': originalfile,\n",
    "            'filename': time + filename\n",
    "        }\n",
    "\n",
    "        response = requests.post(results_url, args)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2775ceb",
   "metadata": {},
   "source": [
    "***READING DATAFRAME***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CREATE SPARK SESSION ############################ ENTER YOUR SPARK MASTER IP AND PORT TO CONNECT TO SERVER ################from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('spark://0.0.0.0:0000').getOrCreate()\n",
    "#%run dp3dpHooks.ipynb\n",
    "try:\n",
    "\t#sourcePreExecutionHook()\n",
    "\n",
    "\tdpfdata = HDFSConnector.fetch(spark, \"{'url': '/FileStore/platform/uploadedSourceFiles/part-00000-2eb316a4-5021-473b-9fc5-ed4b95d91c7e-c000.csv', 'filename': '1674548563DP2FData.csv', 'delimiter': ',', 'file_type': 'Delimeted', 'dbfs_token': '', 'dbfs_domain': '', 'FilePath': '/dataPipeline/DP2FData.csv', 'viewFileName': 'DP2FData.csv', 'is_header': 'Use Header Line', 'baseType': 'hdfs', 'server_url': '/numtraPlatform/NumtraPlatformV2/uploads/platform/', 'results_url': 'http://dev.numtra.com:4040/api/read/hdfs'}\")\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9d6ff",
   "metadata": {},
   "source": [
    "***PERFORMING OPERATIONS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3116260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run dp3dpHooks.ipynb\n",
    "try:\n",
    "\t#operationPreExecutionHook()\n",
    "\n",
    "datapreparation = runDataCleansing(dpfdata,spark,json.dumps( {\"url\": \"/FileStore/platform/uploadedSourceFiles/part-00000-2eb316a4-5021-473b-9fc5-ed4b95d91c7e-c000.csv\", \"source_attributes\": {}, \"DataPrepFile\": \"/FileStore/platform/uploadedSourceFiles/part-00000-2eb316a4-5021-473b-9fc5-ed4b95d91c7e-c000.csv\", \"data_source\": \"platfiles\", \"startListenerOnly\": 1, \"dateColumnNames\": [\"due_date\", \"paid_off_time\"], \"FilePath\": \"/FileStore/platform/extra/63cf9e5cc6466544d4d4a2081674551419/0part.csv\", \"requestRatio\": 0.0, \"totalRows\": 6, \"BasicStats\": {\"missingValues\": 278.3, \"numberOfColumns\": 4, \"numberOfRows\": 6, \"duplicateRowCount\": 0, \"stats\": [{\"column\": \"terms\", \"alias\": \"terms\", \"generated\": 0, \"type\": \"numeric\", \"max\": 30, \"min\": 15, \"mean\": 25.0, \"missing\": 0.0, \"stddev\": 7.75, \"outliers\": [], \"validation\": []}, {\"column\": \"due_date\", \"alias\": \"due_date\", \"generated\": 0, \"type\": \"date\", \"max\": \"2016108\", \"min\": \"2016922\", \"mean\": \"\", \"missing\": 0.0, \"stddev\": \"\", \"outliers\": [], \"validation\": \"[]\"}, {\"column\": \"paid_off_time\", \"alias\": \"paid_off_time\", \"generated\": 0, \"type\": \"date\", \"max\": \"2016107\", \"min\": \"2016914\", \"mean\": \"\", \"missing\": 16.7, \"stddev\": \"\", \"outliers\": [], \"validation\": \"[{\\\"1\\\": NaN}]\"}, {\"column\": \"Gender\", \"alias\": \"Gender\", \"generated\": 0, \"type\": \"String\", \"max\": \"male\", \"min\": \"female\", \"mean\": \"\", \"missing\": 0.0, \"stddev\": \"\", \"outliers\": [], \"validation\": []}]}, \"predictionPowerScore\": [{\"Gender\": 1.0, \"due_date\": 0.0, \"paid_off_time\": 0.0, \"terms\": 0.0}, {\"Gender\": 0.0, \"due_date\": 1.0, \"paid_off_time\": 0.0, \"terms\": 0.0}, {\"Gender\": 0.0, \"due_date\": 0.0, \"paid_off_time\": 1.0, \"terms\": 0.0}, {\"Gender\": 0.4375, \"due_date\": 1.0, \"paid_off_time\": 0.0, \"terms\": 1.0}], \"HasBasicStats\": 1, \"functionsApplied\": [{\"functionName\": \"aggregate\", \"applyOn\": [{\"columnName\": \"due_date\", \"type\": \"date\", \"min\": \"2016922\", \"max\": \"2016108\", \"mean\": \"-\"}], \"functionsData\": [{\"aggregateFunction\": \"\", \"aggregateOn\": [{\"columnName\": \"due_date\", \"type\": \"date\", \"min\": \"2016922\", \"max\": \"2016108\", \"mean\": \"-\"}], \"groupOn\": [{\"columnName\": \"Gender\", \"type\": \"String\", \"min\": \"female\", \"max\": \"male\", \"mean\": \"-\"}]}]}], \"functionChanges\": [{\"columnName\": \"due_date\", \"functionName\": \"Aggregate\", \"Type\": \"date\", \"Parameters\": [{\"aggregateFunction\": \"\", \"aggregateOn\": [{\"columnName\": \"due_date\", \"type\": \"date\", \"min\": \"2016922\", \"max\": \"2016108\", \"mean\": \"-\"}], \"groupOn\": [{\"columnName\": \"Gender\", \"type\": \"String\", \"min\": \"female\", \"max\": \"male\", \"mean\": \"-\"}]}]}], \"fileheader\": [{\"field\": \"terms\", \"alias\": \"terms\", \"generated\": 0, \"position\": 1, \"type\": \"numeric\"}, {\"field\": \"due_date\", \"alias\": \"due_date\", \"generated\": 0, \"position\": 2, \"type\": \"date\"}, {\"field\": \"paid_off_time\", \"alias\": \"paid_off_time\", \"generated\": 0, \"position\": 3, \"type\": \"date\"}, {\"field\": \"Gender\", \"alias\": \"Gender\", \"generated\": 0, \"position\": 4, \"type\": \"String\"}]}))\n",
    "\t#operationPostExecutionHook(datapreparation)\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c754c4",
   "metadata": {},
   "source": [
    "***WRITING DATAFRAME***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run dp3dpHooks.ipynb\n",
    "try:\n",
    "\t#sinkPreExecutionHook()\n",
    "\n",
    "\tdatapipeline = NumtraConnector.fetch(spark, \"{'samplefile': '/FileStore/platform/sampleData/63cf9e59c6466544d4d4a204/part-00000-3330261e-8fdb-4d19-9cf4-47709506e0e5-c000.csv', 'samplecount': 6, 'originalcount': 6, 'orignalKey': '/FileStore/platform/uploadedSourceFiles/part-00000-2eb316a4-5021-473b-9fc5-ed4b95d91c7e-c000.csv', 'pathOnly': '/dataPipeline', 'project_id': '63c0fe048b96720de22fd1f9', 'parent_id': '63c0fe048b96720de22fd1f9', 'original_schema': [{'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'terms', 'alias': 'terms', 'type': 'numeric', 'position': '0'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'due_date', 'alias': 'due_date', 'type': 'date', 'position': '1'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'paid_off_time', 'alias': 'paid_off_time', 'type': 'date', 'position': '2'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'Gender', 'alias': 'Gender', 'type': 'String', 'position': '3'}], 'actual_schema': [{'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'terms', 'alias': 'terms', 'type': 'numeric', 'position': '0'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'due_date', 'alias': 'due_date', 'type': 'numeric', 'position': '1'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'paid_off_time', 'alias': 'paid_off_time', 'type': 'real', 'position': '2'}, {'checked': True, 'inherited': True, 'is_categorical': False, 'bad_values': '', 'nullable': 'true', 'field': 'Gender', 'alias': 'Gender', 'type': 'String', 'position': '3'}], 'server': 'https://dev.numtra.com:443', 'server_url': '/numtraPlatform/NumtraPlatformV2/uploads/platform/', 'delimiter': ',', 'file_type': 'Delimeted', 'filename': 'DP3DPData.csv', 'token': '', 'domain': '', 'is_header': 'Use Header Line', 'url': '/FileStore/platform/uploadedSourceFiles/part-00000-353ad0b2-6826-493a-b81d-aaa1e22f0703-c000.csv', 'results_url': 'http://dev.numtra.com:4040/api/read/hdfs'}\")\n",
    "\n",
    "except Exception as ex: \n",
    "\tlogging.error(ex)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
